{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66e2be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoConfig, Wav2Vec2FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61a12b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\": \"../../KEMDy20_v1_1/Splitting/Train.csv\",\n",
    "    \"test\": \"../../KEMDy20_v1_1/Splitting/Test.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1383cd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Yechani/.cache/huggingface/datasets/csv/default-1ac06c035d845d89/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d126a64f3847a68197d7d02e24349e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Emotion', 'Path'],\n",
      "    num_rows: 25890\n",
      "})\n",
      "Dataset({\n",
      "    features: ['Emotion', 'Path'],\n",
      "    num_rows: 6312\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files = data_files)\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "003fa920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "from transformers.file_utils import ModelOutput\n",
    "\n",
    "@dataclass\n",
    "class SpeechClassifierOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81d6ff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2PreTrainedModel,\n",
    "    Wav2Vec2Model\n",
    ")\n",
    "\n",
    "\n",
    "class Wav2Vec2ClassificationHead(nn.Module):\n",
    "    \"\"\"Head for wav2vec classification task.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.pooling_mode = config.pooling_mode\n",
    "        self.config = config\n",
    "\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.classifier = Wav2Vec2ClassificationHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def freeze_feature_extractor(self):\n",
    "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "\n",
    "    def merged_strategy(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            mode=\"mean\"\n",
    "    ):\n",
    "        if mode == \"mean\":\n",
    "            outputs = torch.mean(hidden_states, dim=1)\n",
    "        elif mode == \"sum\":\n",
    "            outputs = torch.sum(hidden_states, dim=1)\n",
    "        elif mode == \"max\":\n",
    "            outputs = torch.max(hidden_states, dim=1)[0]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_values,\n",
    "            attention_mask=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "            labels=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.wav2vec2(\n",
    "            input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SpeechClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20c2fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name_or_path = \"../../KEMDy20_v1_1/Pretrained_Model2/\"\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name_or_path)\n",
    "sampling_rate = feature_extractor.sampling_rate\n",
    "model = Wav2Vec2ForSpeechClassification.from_pretrained(model_name_or_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fa51cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_file_to_array_fn(batch):\n",
    "    speech_array, sampling_rate = torchaudio.load(batch[\"Path\"])\n",
    "    resampler = torchaudio.transforms.Resample(sampling_rate)\n",
    "    speech = resampler(speech_array).squeeze().numpy()[::-1]\n",
    "\n",
    "    batch[\"speech\"] = speech\n",
    "    return batch\n",
    "\n",
    "def predict(batch):\n",
    "    features = feature_extractor(batch[\"speech\"], sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    input_values = features.input_values.to(device)\n",
    "    attention_mask = features.attention_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values, attention_mask=attention_mask).logits \n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    batch[\"predicted\"] = pred_ids\n",
    "    return batch\n",
    "\n",
    "def predict_logits(batch):\n",
    "    features = feature_extractor(batch[\"speech\"], sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    input_values = features.input_values.to(device)\n",
    "    attention_mask = features.attention_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values, attention_mask=attention_mask).logits \n",
    "\n",
    "    pred_ids = logits.detach().cpu().numpy()\n",
    "    batch[\"predicted\"] = pred_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a28fa37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25890 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(speech_file_to_array_fn)\n",
    "test_dataset = test_dataset.map(speech_file_to_array_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0933984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25890 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_logits = train_dataset.map(predict_logits, batched=True, batch_size=8)\n",
    "test_logits = test_dataset.map(predict_logits, batched=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40a6adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2vec_train_logits = np.array(train_logits[\"predicted\"])\n",
    "wav2vec_train_pred = np.exp(wav2vec_train_logits)/np.sum(np.exp(wav2vec_train_logits), axis=1, keepdims=True)\n",
    "\n",
    "wav2vec_test_logits = np.array(test_logits[\"predicted\"])\n",
    "wav2vec_test_pred = np.exp(wav2vec_test_logits)/np.sum(np.exp(wav2vec_test_logits), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ff830d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"../../KEMDy20_v1_1/wav2vec_R_train_pred.npz\", predict_prob=wav2vec_train_pred)\n",
    "np.savez(\"../../KEMDy20_v1_1/wav2vec_R_test_pred.npz\", predict_prob=wav2vec_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2fd56a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "kobert_test_pred = np.load(\"../../KEMDy20_v1_1/kobert_test_pred.npz\")[\"predict_prob\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab05df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2vec_bert_test_pred = (wav2vec_test_pred*0.5 + kobert_test_pred*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "59ae0a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['angry', 'disqust', 'fear', 'happy', 'neutral', 'sad', 'surprise']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = [config.id2label[i] for i in range(config.num_labels)]\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62bee410",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [config.label2id[name] for name in test_logits[\"Emotion\"]]\n",
    "y_wav2vec_pred = np.argmax(wav2vec_test_pred, axis=1)\n",
    "y_bert_pred = np.argmax(kobert_test_pred, axis=1)\n",
    "y_wav2vec_bert_pred = np.argmax(wav2vec_bert_test_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b797fea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.7304    0.6732    0.7006       664\n",
      "     disqust     0.7253    0.7638    0.7440       470\n",
      "        fear     0.7143    0.7732    0.7426       291\n",
      "       happy     0.7725    0.7465    0.7592      1128\n",
      "     neutral     0.8719    0.9186    0.8947      2741\n",
      "         sad     0.7766    0.6852    0.7280       629\n",
      "    surprise     0.8067    0.7404    0.7721       389\n",
      "\n",
      "    accuracy                         0.8096      6312\n",
      "   macro avg     0.7711    0.7573    0.7630      6312\n",
      "weighted avg     0.8075    0.8096    0.8077      6312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_wav2vec_pred, target_names=label_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bcbacfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.9038    0.8343    0.8677       664\n",
      "     disqust     0.9085    0.8872    0.8977       470\n",
      "        fear     0.9498    0.9107    0.9298       291\n",
      "       happy     0.8971    0.8271    0.8607      1128\n",
      "     neutral     0.8734    0.9486    0.9094      2741\n",
      "         sad     0.8961    0.8362    0.8651       629\n",
      "    surprise     0.9468    0.8689    0.9062       389\n",
      "\n",
      "    accuracy                         0.8924      6312\n",
      "   macro avg     0.9108    0.8733    0.8909      6312\n",
      "weighted avg     0.8937    0.8924    0.8918      6312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_bert_pred, target_names=label_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86e2bbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry     0.9132    0.8238    0.8662       664\n",
      "     disqust     0.8989    0.8894    0.8941       470\n",
      "        fear     0.9597    0.9003    0.9291       291\n",
      "       happy     0.9322    0.8165    0.8705      1128\n",
      "     neutral     0.8704    0.9726    0.9187      2741\n",
      "         sad     0.9045    0.8283    0.8647       629\n",
      "    surprise     0.9569    0.8560    0.9037       389\n",
      "\n",
      "    accuracy                         0.8980      6312\n",
      "   macro avg     0.9194    0.8696    0.8924      6312\n",
      "weighted avg     0.9009    0.8980    0.8969      6312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_wav2vec_bert_pred, target_names=label_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889943a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
