{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["## Drive Mount\n"],"metadata":{"id":"wNtragTlpA9A"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"KbUvrXExpEnw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Working directory"],"metadata":{"id":"n8393-P_pO46"}},{"cell_type":"code","source":["import os\n","os.chdir('./drive/MyDrive/KEMD/code')\n","os.getcwd()"],"metadata":{"id":"X2-C8BdzpQUd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train / Test file"],"metadata":{"id":"7RlnvyMcpSeu"}},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"8MoHnXPBpTs4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train\n","train =pd.read_csv('../data/Train.csv',encoding='euc-kr')\n","#Test\n","test=pd.read_csv('../data/Test.csv',encoding='euc-kr')"],"metadata":{"id":"yX-CsrXGpVdC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Package installation required"],"metadata":{"id":"na7Td3bfpWkM"}},{"cell_type":"code","source":["!pip install mxnet\n","!pip install gluonnlp==0.8.0\n","!pip install pandas tqdm\n","!pip install sentencepiece\n","!pip install transformers\n","!pip install torch     "],"metadata":{"id":"43ka020VpX4X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hugging Face API로 KoBERT 다운로드\n","!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"],"metadata":{"id":"bgPozfBCw_zX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Package Load"],"metadata":{"id":"Rjejws0Lpe61"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook"],"metadata":{"id":"r-FFM9fupf-i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip3 install kobert-transformers"],"metadata":{"id":"586tpJNokcPg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ★ Hugging Face를 통한 모델 및 토크나이저 Import\n","from kobert_tokenizer import KoBERTTokenizer\n","from transformers import BertModel\n","# ●\n","#from transformers import BertTokenizer\n","\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup"],"metadata":{"id":"I-cVT1C-sfxG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#GPU 설정\n","device = torch.device(\"cuda:0\")"],"metadata":{"id":"FhtQV1-tpkO0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#BERT, Vocabulary 불러오기\n","#bertmodel, vocab = get_pytorch_kobert_model()"],"metadata":{"id":"VhL1wDP-plr9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ★\n","tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n","bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n","vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')"],"metadata":{"id":"326aDugEkgzu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Label Encoding"],"metadata":{"id":"9xlhi67dpnyN"}},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","from keras.utils import to_categorical"],"metadata":{"id":"WsEvNMi8ppII"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 레이블 인코딩\n","le = LabelEncoder()\n","le.fit(train.Emotion.values)\n","encoded_train_y = le.transform(train.Emotion.values)\n","\n","# train_y에서 사용된 LabelEncoder 객체를 그대로 사용하여 test_y 인코딩\n","encoded_test_y = le.transform(test.Emotion.values)\n","\n","train[\"Emotion\"] = encoded_train_y\n","\n","test[\"Emotion\"] = encoded_test_y"],"metadata":{"id":"Ypc4aC5RpqKP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Bert Train / Test set\n","\n"],"metadata":{"id":"0qfJdsnppsna"}},{"cell_type":"code","source":["#Bert Train set\n","train_data = []\n","for q, label in zip(train['Text'],train['Emotion']):\n","    data =[]\n","    data.append(q)\n","    data.append(str(label))\n","\n","    train_data.append(data)\n","\n","#Bert Test set\n","test_data=[]\n","for q, label in zip(test['Text'],test['Emotion']):\n","    data_1 =[]\n","    data_1.append(q)\n","    data_1.append(str(label))\n","\n","    test_data.append(data_1)"],"metadata":{"id":"AXj5yoTrpvev"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Bert Input Dataset class"],"metadata":{"id":"eg97ukHdpyUF"}},{"cell_type":"code","source":["class BERTSentenceTransform:\n","    r\"\"\"BERT style data transformation.\n","\n","    Parameters\n","    ----------\n","    tokenizer : BERTTokenizer.\n","        Tokenizer for the sentences.\n","    max_seq_length : int.\n","        Maximum sequence length of the sentences.\n","    pad : bool, default True\n","        Whether to pad the sentences to maximum length.\n","    pair : bool, default True\n","        Whether to transform sentences or sentence pairs.\n","    \"\"\"\n","\n","    def __init__(self, tokenizer, max_seq_length,vocab, pad=True, pair=True):\n","        self._tokenizer = tokenizer\n","        self._max_seq_length = max_seq_length\n","        self._pad = pad\n","        self._pair = pair\n","        self._vocab = vocab \n","\n","    def __call__(self, line):\n","        \"\"\"Perform transformation for sequence pairs or single sequences.\n","\n","        The transformation is processed in the following steps:\n","        - tokenize the input sequences\n","        - insert [CLS], [SEP] as necessary\n","        - generate type ids to indicate whether a token belongs to the first\n","        sequence or the second sequence.\n","        - generate valid length\n","\n","        For sequence pairs, the input is a tuple of 2 strings:\n","        text_a, text_b.\n","\n","        Inputs:\n","            text_a: 'is this jacksonville ?'\n","            text_b: 'no it is not'\n","        Tokenization:\n","            text_a: 'is this jack ##son ##ville ?'\n","            text_b: 'no it is not .'\n","        Processed:\n","            tokens: '[CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]'\n","            type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n","            valid_length: 14\n","\n","        For single sequences, the input is a tuple of single string:\n","        text_a.\n","\n","        Inputs:\n","            text_a: 'the dog is hairy .'\n","        Tokenization:\n","            text_a: 'the dog is hairy .'\n","        Processed:\n","            text_a: '[CLS] the dog is hairy . [SEP]'\n","            type_ids: 0     0   0   0  0     0 0\n","            valid_length: 7\n","\n","        Parameters\n","        ----------\n","        line: tuple of str\n","            Input strings. For sequence pairs, the input is a tuple of 2 strings:\n","            (text_a, text_b). For single sequences, the input is a tuple of single\n","            string: (text_a,).\n","\n","        Returns\n","        -------\n","        np.array: input token ids in 'int32', shape (batch_size, seq_length)\n","        np.array: valid length in 'int32', shape (batch_size,)\n","        np.array: input token type ids in 'int32', shape (batch_size, seq_length)\n","\n","        \"\"\"\n","\n","        # convert to unicode\n","        text_a = line[0]\n","        if self._pair:\n","            assert len(line) == 2\n","            text_b = line[1]\n","\n","        tokens_a = self._tokenizer.tokenize(text_a)\n","        tokens_b = None\n","\n","        if self._pair:\n","            tokens_b = self._tokenizer(text_b)\n","\n","        if tokens_b:\n","            # Modifies `tokens_a` and `tokens_b` in place so that the total\n","            # length is less than the specified length.\n","            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n","            self._truncate_seq_pair(tokens_a, tokens_b,\n","                                    self._max_seq_length - 3)\n","        else:\n","            # Account for [CLS] and [SEP] with \"- 2\"\n","            if len(tokens_a) > self._max_seq_length - 2:\n","                tokens_a = tokens_a[0:(self._max_seq_length - 2)]\n","\n","        # The embedding vectors for `type=0` and `type=1` were learned during\n","        # pre-training and are added to the wordpiece embedding vector\n","        # (and position vector). This is not *strictly* necessary since\n","        # the [SEP] token unambiguously separates the sequences, but it makes\n","        # it easier for the model to learn the concept of sequences.\n","\n","        # For classification tasks, the first vector (corresponding to [CLS]) is\n","        # used as as the \"sentence vector\". Note that this only makes sense because\n","        # the entire model is fine-tuned.\n","        #vocab = self._tokenizer.vocab\n","        vocab = self._vocab\n","        tokens = []\n","        tokens.append(vocab.cls_token)\n","        tokens.extend(tokens_a)\n","        tokens.append(vocab.sep_token)\n","        segment_ids = [0] * len(tokens)\n","\n","        if tokens_b:\n","            tokens.extend(tokens_b)\n","            tokens.append(vocab.sep_token)\n","            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))\n","\n","        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # The valid length of sentences. Only real  tokens are attended to.\n","        valid_length = len(input_ids)\n","\n","        if self._pad:\n","            # Zero-pad up to the sequence length.\n","            padding_length = self._max_seq_length - valid_length\n","            # use padding tokens for the rest\n","            input_ids.extend([vocab[vocab.padding_token]] * padding_length)\n","            segment_ids.extend([0] * padding_length)\n","\n","        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\\\n","            np.array(segment_ids, dtype='int32')"],"metadata":{"id":"yxBqMaBOpz0f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ★\n","class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,\n","                 pad, pair):\n","        transform = BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n","        #transform = nlp.data.BERTSentenceTransform(\n","        #    tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))\n"],"metadata":{"id":"KXPTP1QGkuzU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Bert hyperparameters"],"metadata":{"id":"CGz8L7Xvp1UT"}},{"cell_type":"code","source":["# Bert hyperparameters\n","max_len = 64\n","batch_size = 64\n","warmup_ratio = 0.1\n","num_epochs = 7\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate =  2e-05\n","dr_rate = 0.5"],"metadata":{"id":"bhGwxAZLp3Bn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train Tokenization"],"metadata":{"id":"W97OCY0dp41-"}},{"cell_type":"code","source":["train_dataset = BERTDataset(train_data, 0, 1, tokenizer, vocab, max_len, True, False)\n","\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,num_workers=2)"],"metadata":{"id":"WvzUKcUGp6Um"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## KoBERT"],"metadata":{"id":"RPMfrUR3p9et"}},{"cell_type":"code","source":["class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768,\n","                 num_classes=7,   ##클래스 수 조정##\n","                 dr_rate=dr_rate,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","        \n","                 \n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","    \n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        \n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)\n","\n","    def extract_features(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        \n","        output, _ = self.bert(input_ids=token_ids, token_type_ids=segment_ids.long(), attention_mask=attention_mask.float().to(token_ids.device))\n","        return output"],"metadata":{"id":"YW0yNKW5qAdA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = BERTClassifier(bertmodel,  dr_rate=dr_rate).to(device)"],"metadata":{"id":"7i_hDhqPqA4Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Optimizar & Schedule"],"metadata":{"id":"-Ug8Iva1qCPu"}},{"cell_type":"code","source":["#Optimizar : AdamW\n","no_decay = ['bias', 'LayerNorm.weight']\n","\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","\n","optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","\n","#Loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","#Scheduler\n","t_total = len(train_dataloader) * num_epochs\n","\n","warmup_step = int(t_total * warmup_ratio)\n","\n","scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"],"metadata":{"id":"IfUHfiPOqDSv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Metric : Accuracy"],"metadata":{"id":"ODsEPRkUqFp3"}},{"cell_type":"code","source":["def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc"],"metadata":{"id":"a6U3vy-5qG0O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Train"],"metadata":{"id":"45Z_8uLBqIfI"}},{"cell_type":"code","source":["#epoch 7\n","for e in range(num_epochs):\n","    train_acc = 0.0\n","    model.train()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n","        optimizer.zero_grad()\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        loss = loss_fn(out, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","        optimizer.step()\n","        scheduler.step()  # Update learning rate schedule\n","        train_acc += calc_accuracy(out, label)\n","        if batch_id % log_interval == 0:\n","            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n","    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))"],"metadata":{"id":"w2DKAUaJqKXB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model save"],"metadata":{"id":"_qrpcMtYqNG3"}},{"cell_type":"code","source":["model_save_dir = '../model/'\n","if not os.path.exists(model_save_dir):\n","    os.mkdir(model_save_dir)\n","torch.save(model, model_save_dir + 'kobert_train.pt') \n","torch.save(model.state_dict(), model_save_dir + 'kobert_state_dict.pt')  # 모델 객체의 state_dict 저장\n","\n","torch.save({\n","    'model': model.state_dict(),\n","    'optimizer': optimizer.state_dict()\n","}, model_save_dir + 'kobert_emotions_all.tar')"],"metadata":{"id":"MgSQG7ZYqPxo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Load"],"metadata":{"id":"MtljD3LzqTT-"}},{"cell_type":"code","source":["# Train 안돌렸을 때 Optimizar & Schedule 다시 설정\n","# \"\"\"## Optimizar & Schedule\"\"\"\n","\n","# #Optimizar : AdamW\n","# no_decay = ['bias', 'LayerNorm.weight']\n","\n","# optimizer_grouped_parameters = [\n","#     {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","#     {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","# ]\n","\n","# optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","\n","# #Loss function\n","# loss_fn = nn.CrossEntropyLoss()\n","\n","# #Scheduler\n","# t_total = len(train_dataloader) * num_epochs\n","\n","# warmup_step = int(t_total * warmup_ratio)\n","\n","# scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n","\n","# #정확도 측정을 위한 함수 정의\n","# def calc_accuracy(X,Y):\n","#     max_vals, max_indices = torch.max(X, 1)\n","#     train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","#     return train_acc"],"metadata":{"id":"p_5e9IpSqVyY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Model load\n","model = torch.load('../model/kobert_train.pt')  # 전체 모델을 통째로 불러옴, 클래스 선언 필수\n","model.load_state_dict(torch.load('../model/kobert_state_dict.pt'))  # state_dict를 불러 온 후, 모델에 저장\n","\n","checkpoint = torch.load('../model/kobert_emotions_all.tar')   # dict 불러오기\n","model.load_state_dict(checkpoint['model'])\n","optimizer.load_state_dict(checkpoint['optimizer'])"],"metadata":{"id":"cK2nbzZnqXeX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Test function"],"metadata":{"id":"vZ4Lv4yPqakx"}},{"cell_type":"code","source":["def predict(test_data):\n","    another_test = BERTDataset(test_data, 0, 1, tok, max_len, True, False)\n","    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=20)\n","\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","\n","        out = model(token_ids, valid_length, segment_ids)\n","\n","\n","        test_eval=[]\n","        \n","        for i in out:\n","            logits=i\n","            logits = logits.detach().cpu().numpy()\n","            test_eval.append(np.argmax(logits))\n","            for e in len(test_eval):\n","                submission['topic_idx'][e] = test_eval[e]\n","\n","        print(test_eval)"],"metadata":{"id":"XjTi1StHqdbX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train set Predict"],"metadata":{"id":"OnFnxEnoqhiH"}},{"cell_type":"code","source":["# #Train set 예측\n","# train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size)\n","\n","# train_eval=[]\n","# train_prob=[]\n","\n","# for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n","#     token_ids = token_ids.long().to(device)\n","#     segment_ids = segment_ids.long().to(device)\n","\n","#     valid_length= valid_length\n","#     label = label.long().to(device)\n","\n","#     out = model(token_ids, valid_length, segment_ids)\n","\n","#     for i in out:\n","#         logits=i\n","#         logits = logits.detach().cpu().numpy()\n","#         train_eval.append(np.argmax(logits))\n","#         probs = nn.functional.softmax(torch.tensor(logits).reshape(-1, 7), dim=1)\n","#         train_prob.extend(probs.detach().cpu().numpy())\n","\n","# train_probabilty = np.stack(train_prob)\n","\n","# np.savez('predict_train_prob.npz',predict_prob=train_probabilty)"],"metadata":{"id":"GfklSIo_qkAf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Test"],"metadata":{"id":"ho8iHSbaqmMM"}},{"cell_type":"code","source":["test_dataset = BERTDataset(test_data, 0, 1, tokenizer, vocab, max_len, True, False)\n","\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=2)\n","\n","\n","test_eval=[]\n","test_prob=[]\n","\n","for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n","    token_ids = token_ids.long().to(device)\n","    segment_ids = segment_ids.long().to(device)\n","\n","    valid_length= valid_length\n","    label = label.long().to(device)\n","\n","    out = model(token_ids, valid_length, segment_ids)\n","\n","    for i in out:\n","        logits=i\n","        logits = logits.detach().cpu().numpy()\n","        test_eval.append(np.argmax(logits))\n","        probs = nn.functional.softmax(torch.tensor(logits).reshape(-1, 7), dim=1)\n","        test_prob.extend(probs.detach().cpu().numpy())"],"metadata":{"id":"y2gWyehyqoUN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model evaluation"],"metadata":{"id":"iudCqVaVqqw8"}},{"cell_type":"code","source":["# 예측값\n","predict= pd.DataFrame(test_eval)\n","\n","y_pred = list(predict[0])\n","\n","#predict.to_csv('kobert_predict.csv')\n","\n","# 실제값 \n","y_true = list(test['Emotion'])\n","\n","#Classification Report Test 성능 확인\n","from sklearn.metrics import classification_report\n","\n","print(classification_report(y_true, y_pred))"],"metadata":{"id":"kwpNEpb0qsuR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"### test 예측확률 저장\"\"\"\n","\n","test_probabilty = np.stack(test_prob)\n","\n","np.savez('KoBERT_'+str(dr_rate)[0]+str(dr_rate)[2]+'.npz',predict_prob=test_probabilty)\n","\n","# data = np.load('predict_prob.npz')"],"metadata":{"id":"QWzzzKTqqu3A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Embadding Visualization"],"metadata":{"id":"UCPagNP6qxgl"}},{"cell_type":"code","source":["from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt"],"metadata":{"id":"NBSgmxHGq1aV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eatures = []\n","\n","with torch.no_grad():\n","    model.eval()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        feature = model.extract_features(token_ids, valid_length, segment_ids)\n","        features.append(feature.detach().cpu().numpy())\n","\n","features = np.concatenate(features, axis=0)\n","\n","# feature shape을 (6312, 64*768)으로 reshape\n","features_2d = features.reshape((features.shape[0], -1))"],"metadata":{"id":"KrIrB59lq2na"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# T-SNE 모델 생성 및 학습\n","tsne = TSNE(n_components=2, random_state=0)\n","features_tsne = tsne.fit_transform(features_2d)"],"metadata":{"id":"neyBdhJaq4q0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Embadding Visulaztion\n","cmap = plt.cm.get_cmap('jet', 7)\n","emotion_list = ['angry', 'disqust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n","plt.scatter(features_tsne[:, 0], features_tsne[:, 1], c=test['Emotion'], cmap=cmap, s=5)\n","\n","# legend 달기\n","for i, label in enumerate(np.unique(test['Emotion'])):\n","    plt.scatter([], [], c=cmap(i), label=emotion_list[label], s=30)\n","\n","# legend 위치 조정\n","plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n","plt.show()\n","#plt.savefig('t-SNE_word_embadding visualization_test.jpg')"],"metadata":{"id":"dFNDWOawq8Fq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pretrained Model Embadding Visualization"],"metadata":{"id":"CwLu1KI5q-_9"}},{"cell_type":"code","source":["no_train_bert,no_train_vocab = get_pytorch_kobert_model()\n","\n","no_train_model = BERTClassifier(no_train_bert,  dr_rate=0.5).to(device)"],"metadata":{"id":"6OL0D-3DrEU6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eatures_1 = []\n","\n","with torch.no_grad():\n","    model.eval()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        feature = no_train_model.extract_features(token_ids, valid_length, segment_ids)\n","        features_1.append(feature.detach().cpu().numpy())\n","\n","features_1 = np.concatenate(features_1, axis=0)\n","\n","# feature shape을 (6312, 64*768)으로 reshape\n","features_2d_1 = features_1.reshape((features_1.shape[0], -1))"],"metadata":{"id":"oaRtW-pSrG9l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# T-SNE 모델 생성 및 학습\n","tsne = TSNE(n_components=2, random_state=0)\n","features_tsne_1 = tsne.fit_transform(features_2d_1)"],"metadata":{"id":"YYLQV1-ErHTm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Embadding Visulaztion\n","cmap = plt.cm.get_cmap('jet', 7)\n","emotion_list = ['angry', 'disqust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n","plt.scatter(features_tsne_1[:, 0], features_tsne_1[:, 1], c=test['Emotion'], cmap=cmap, s=5)\n","\n","# legend 달기\n","for i, label in enumerate(np.unique(test['Emotion'])):\n","    plt.scatter([], [], c=cmap(i), label=emotion_list[label], s=30)\n","\n","# legend 위치 조정\n","plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n","plt.show()\n","#plt.savefig('t-SNE_word_embadding visualization_pretrained_model_test.jpg')"],"metadata":{"id":"A-TzNIagrIeT"},"execution_count":null,"outputs":[]}]}