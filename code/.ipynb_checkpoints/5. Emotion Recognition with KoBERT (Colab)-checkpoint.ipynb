{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNtragTlpA9A"
   },
   "source": [
    "## Drive Mount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbUvrXExpEnw"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8393-P_pO46"
   },
   "source": [
    "## Working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2-C8BdzpQUd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('./drive/MyDrive/KEMD/code')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RlnvyMcpSeu"
   },
   "source": [
    "## Train / Test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MoHnXPBpTs4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yX-CsrXGpVdC"
   },
   "outputs": [],
   "source": [
    "#Train\n",
    "train =pd.read_csv('../data/Train.csv',encoding='euc-kr')\n",
    "#Test\n",
    "test=pd.read_csv('../data/Test.csv',encoding='euc-kr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "na7Td3bfpWkM"
   },
   "source": [
    "## Package installation required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43ka020VpX4X"
   },
   "outputs": [],
   "source": [
    "!pip install mxnet\n",
    "!pip install gluonnlp==0.8.0\n",
    "!pip install pandas tqdm\n",
    "!pip install sentencepiece\n",
    "!pip install transformers\n",
    "!pip install torch     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgPozfBCw_zX"
   },
   "outputs": [],
   "source": [
    "# Hugging Face API로 KoBERT 다운로드\n",
    "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rjejws0Lpe61"
   },
   "source": [
    "## Package Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-FFM9fupf-i"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "586tpJNokcPg"
   },
   "outputs": [],
   "source": [
    "!pip3 install kobert-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-cVT1C-sfxG"
   },
   "outputs": [],
   "source": [
    "# ★ Hugging Face를 통한 모델 및 토크나이저 Import\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "from transformers import BertModel\n",
    "# ●\n",
    "#from transformers import BertTokenizer\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhtQV1-tpkO0"
   },
   "outputs": [],
   "source": [
    "#GPU 설정\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhL1wDP-plr9"
   },
   "outputs": [],
   "source": [
    "#BERT, Vocabulary 불러오기\n",
    "#bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "326aDugEkgzu"
   },
   "outputs": [],
   "source": [
    "# ★\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xlhi67dpnyN"
   },
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WsEvNMi8ppII"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ypc4aC5RpqKP"
   },
   "outputs": [],
   "source": [
    "# 레이블 인코딩\n",
    "le = LabelEncoder()\n",
    "le.fit(train.Emotion.values)\n",
    "encoded_train_y = le.transform(train.Emotion.values)\n",
    "\n",
    "# train_y에서 사용된 LabelEncoder 객체를 그대로 사용하여 test_y 인코딩\n",
    "encoded_test_y = le.transform(test.Emotion.values)\n",
    "\n",
    "train[\"Emotion\"] = encoded_train_y\n",
    "\n",
    "test[\"Emotion\"] = encoded_test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qfJdsnppsna"
   },
   "source": [
    "## Bert Train / Test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXj5yoTrpvev"
   },
   "outputs": [],
   "source": [
    "#Bert Train set\n",
    "train_data = []\n",
    "for q, label in zip(train['Text'],train['Emotion']):\n",
    "    data =[]\n",
    "    data.append(q)\n",
    "    data.append(str(label))\n",
    "\n",
    "    train_data.append(data)\n",
    "\n",
    "#Bert Test set\n",
    "test_data=[]\n",
    "for q, label in zip(test['Text'],test['Emotion']):\n",
    "    data_1 =[]\n",
    "    data_1.append(q)\n",
    "    data_1.append(str(label))\n",
    "\n",
    "    test_data.append(data_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eg97ukHdpyUF"
   },
   "source": [
    "## Bert Input Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yxBqMaBOpz0f"
   },
   "outputs": [],
   "source": [
    "class BERTSentenceTransform:\n",
    "    r\"\"\"BERT style data transformation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenizer : BERTTokenizer.\n",
    "        Tokenizer for the sentences.\n",
    "    max_seq_length : int.\n",
    "        Maximum sequence length of the sentences.\n",
    "    pad : bool, default True\n",
    "        Whether to pad the sentences to maximum length.\n",
    "    pair : bool, default True\n",
    "        Whether to transform sentences or sentence pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, max_seq_length,vocab, pad=True, pair=True):\n",
    "        self._tokenizer = tokenizer\n",
    "        self._max_seq_length = max_seq_length\n",
    "        self._pad = pad\n",
    "        self._pair = pair\n",
    "        self._vocab = vocab \n",
    "\n",
    "    def __call__(self, line):\n",
    "        \"\"\"Perform transformation for sequence pairs or single sequences.\n",
    "\n",
    "        The transformation is processed in the following steps:\n",
    "        - tokenize the input sequences\n",
    "        - insert [CLS], [SEP] as necessary\n",
    "        - generate type ids to indicate whether a token belongs to the first\n",
    "        sequence or the second sequence.\n",
    "        - generate valid length\n",
    "\n",
    "        For sequence pairs, the input is a tuple of 2 strings:\n",
    "        text_a, text_b.\n",
    "\n",
    "        Inputs:\n",
    "            text_a: 'is this jacksonville ?'\n",
    "            text_b: 'no it is not'\n",
    "        Tokenization:\n",
    "            text_a: 'is this jack ##son ##ville ?'\n",
    "            text_b: 'no it is not .'\n",
    "        Processed:\n",
    "            tokens: '[CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]'\n",
    "            type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "            valid_length: 14\n",
    "\n",
    "        For single sequences, the input is a tuple of single string:\n",
    "        text_a.\n",
    "\n",
    "        Inputs:\n",
    "            text_a: 'the dog is hairy .'\n",
    "        Tokenization:\n",
    "            text_a: 'the dog is hairy .'\n",
    "        Processed:\n",
    "            text_a: '[CLS] the dog is hairy . [SEP]'\n",
    "            type_ids: 0     0   0   0  0     0 0\n",
    "            valid_length: 7\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        line: tuple of str\n",
    "            Input strings. For sequence pairs, the input is a tuple of 2 strings:\n",
    "            (text_a, text_b). For single sequences, the input is a tuple of single\n",
    "            string: (text_a,).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array: input token ids in 'int32', shape (batch_size, seq_length)\n",
    "        np.array: valid length in 'int32', shape (batch_size,)\n",
    "        np.array: input token type ids in 'int32', shape (batch_size, seq_length)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # convert to unicode\n",
    "        text_a = line[0]\n",
    "        if self._pair:\n",
    "            assert len(line) == 2\n",
    "            text_b = line[1]\n",
    "\n",
    "        tokens_a = self._tokenizer.tokenize(text_a)\n",
    "        tokens_b = None\n",
    "\n",
    "        if self._pair:\n",
    "            tokens_b = self._tokenizer(text_b)\n",
    "\n",
    "        if tokens_b:\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            self._truncate_seq_pair(tokens_a, tokens_b,\n",
    "                                    self._max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > self._max_seq_length - 2:\n",
    "                tokens_a = tokens_a[0:(self._max_seq_length - 2)]\n",
    "\n",
    "        # The embedding vectors for `type=0` and `type=1` were learned during\n",
    "        # pre-training and are added to the wordpiece embedding vector\n",
    "        # (and position vector). This is not *strictly* necessary since\n",
    "        # the [SEP] token unambiguously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        #vocab = self._tokenizer.vocab\n",
    "        vocab = self._vocab\n",
    "        tokens = []\n",
    "        tokens.append(vocab.cls_token)\n",
    "        tokens.extend(tokens_a)\n",
    "        tokens.append(vocab.sep_token)\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens.extend(tokens_b)\n",
    "            tokens.append(vocab.sep_token)\n",
    "            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))\n",
    "\n",
    "        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The valid length of sentences. Only real  tokens are attended to.\n",
    "        valid_length = len(input_ids)\n",
    "\n",
    "        if self._pad:\n",
    "            # Zero-pad up to the sequence length.\n",
    "            padding_length = self._max_seq_length - valid_length\n",
    "            # use padding tokens for the rest\n",
    "            input_ids.extend([vocab[vocab.padding_token]] * padding_length)\n",
    "            segment_ids.extend([0] * padding_length)\n",
    "\n",
    "        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\\\n",
    "            np.array(segment_ids, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXPTP1QGkuzU"
   },
   "outputs": [],
   "source": [
    "# ★\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,\n",
    "                 pad, pair):\n",
    "        transform = BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
    "        #transform = nlp.data.BERTSentenceTransform(\n",
    "        #    tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGz8L7Xvp1UT"
   },
   "source": [
    "## Bert hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhGwxAZLp3Bn"
   },
   "outputs": [],
   "source": [
    "# Bert hyperparameters\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 7\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  2e-05\n",
    "dr_rate = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W97OCY0dp41-"
   },
   "source": [
    "## Train Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvzUKcUGp6Um"
   },
   "outputs": [],
   "source": [
    "train_dataset = BERTDataset(train_data, 0, 1, tokenizer, vocab, max_len, True, False)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPMfrUR3p9et"
   },
   "source": [
    "## KoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YW0yNKW5qAdA"
   },
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=7,   ##클래스 수 조정##\n",
    "                 dr_rate=dr_rate,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "        \n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "\n",
    "    def extract_features(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        output, _ = self.bert(input_ids=token_ids, token_type_ids=segment_ids.long(), attention_mask=attention_mask.float().to(token_ids.device))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7i_hDhqPqA4Y"
   },
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel,  dr_rate=dr_rate).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ug8Iva1qCPu"
   },
   "source": [
    "## Optimizar & Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfUHfiPOqDSv"
   },
   "outputs": [],
   "source": [
    "#Optimizar : AdamW\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "\n",
    "#Loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "#Scheduler\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODsEPRkUqFp3"
   },
   "source": [
    "## Metric : Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6U3vy-5qG0O"
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45Z_8uLBqIfI"
   },
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2DKAUaJqKXB"
   },
   "outputs": [],
   "source": [
    "#epoch 7\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qrpcMtYqNG3"
   },
   "source": [
    "## Model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MgSQG7ZYqPxo"
   },
   "outputs": [],
   "source": [
    "model_save_dir = '../model/'\n",
    "if not os.path.exists(model_save_dir):\n",
    "    os.mkdir(model_save_dir)\n",
    "torch.save(model, model_save_dir + 'kobert_train.pt') \n",
    "torch.save(model.state_dict(), model_save_dir + 'kobert_state_dict.pt')  # 모델 객체의 state_dict 저장\n",
    "\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "}, model_save_dir + 'kobert_emotions_all.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtljD3LzqTT-"
   },
   "source": [
    "## Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_5e9IpSqVyY"
   },
   "outputs": [],
   "source": [
    "# Train 안돌렸을 때 Optimizar & Schedule 다시 설정\n",
    "# \"\"\"## Optimizar & Schedule\"\"\"\n",
    "\n",
    "# #Optimizar : AdamW\n",
    "# no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "#     {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "# ]\n",
    "\n",
    "# optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "\n",
    "# #Loss function\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# #Scheduler\n",
    "# t_total = len(train_dataloader) * num_epochs\n",
    "\n",
    "# warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "# scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "# #정확도 측정을 위한 함수 정의\n",
    "# def calc_accuracy(X,Y):\n",
    "#     max_vals, max_indices = torch.max(X, 1)\n",
    "#     train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "#     return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cK2nbzZnqXeX"
   },
   "outputs": [],
   "source": [
    "#Model load\n",
    "model = torch.load('../model/kobert_train.pt')  # 전체 모델을 통째로 불러옴, 클래스 선언 필수\n",
    "model.load_state_dict(torch.load('../model/kobert_state_dict.pt'))  # state_dict를 불러 온 후, 모델에 저장\n",
    "\n",
    "checkpoint = torch.load('../model/kobert_emotions_all.tar')   # dict 불러오기\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZ4Lv4yPqakx"
   },
   "source": [
    "## Model Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjTi1StHqdbX"
   },
   "outputs": [],
   "source": [
    "def predict(test_data):\n",
    "    another_test = BERTDataset(test_data, 0, 1, tok, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=20)\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "\n",
    "        test_eval=[]\n",
    "        \n",
    "        for i in out:\n",
    "            logits=i\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            test_eval.append(np.argmax(logits))\n",
    "            for e in len(test_eval):\n",
    "                submission['topic_idx'][e] = test_eval[e]\n",
    "\n",
    "        print(test_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnFnxEnoqhiH"
   },
   "source": [
    "## Train set Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfklSIo_qkAf"
   },
   "outputs": [],
   "source": [
    "# #Train set 예측\n",
    "# train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size)\n",
    "\n",
    "# train_eval=[]\n",
    "# train_prob=[]\n",
    "\n",
    "# for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "#     token_ids = token_ids.long().to(device)\n",
    "#     segment_ids = segment_ids.long().to(device)\n",
    "\n",
    "#     valid_length= valid_length\n",
    "#     label = label.long().to(device)\n",
    "\n",
    "#     out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "#     for i in out:\n",
    "#         logits=i\n",
    "#         logits = logits.detach().cpu().numpy()\n",
    "#         train_eval.append(np.argmax(logits))\n",
    "#         probs = nn.functional.softmax(torch.tensor(logits).reshape(-1, 7), dim=1)\n",
    "#         train_prob.extend(probs.detach().cpu().numpy())\n",
    "\n",
    "# train_probabilty = np.stack(train_prob)\n",
    "\n",
    "# np.savez('predict_train_prob.npz',predict_prob=train_probabilty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ho8iHSbaqmMM"
   },
   "source": [
    "## Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2gWyehyqoUN"
   },
   "outputs": [],
   "source": [
    "test_dataset = BERTDataset(test_data, 0, 1, tokenizer, vocab, max_len, True, False)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "\n",
    "test_eval=[]\n",
    "test_prob=[]\n",
    "\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "\n",
    "    valid_length= valid_length\n",
    "    label = label.long().to(device)\n",
    "\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "    for i in out:\n",
    "        logits=i\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        test_eval.append(np.argmax(logits))\n",
    "        probs = nn.functional.softmax(torch.tensor(logits).reshape(-1, 7), dim=1)\n",
    "        test_prob.extend(probs.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iudCqVaVqqw8"
   },
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwpNEpb0qsuR"
   },
   "outputs": [],
   "source": [
    "# 예측값\n",
    "predict= pd.DataFrame(test_eval)\n",
    "\n",
    "y_pred = list(predict[0])\n",
    "\n",
    "#predict.to_csv('kobert_predict.csv')\n",
    "\n",
    "# 실제값 \n",
    "y_true = list(test['Emotion'])\n",
    "\n",
    "#Classification Report Test 성능 확인\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWzzzKTqqu3A"
   },
   "outputs": [],
   "source": [
    "\"\"\"### test 예측확률 저장\"\"\"\n",
    "\n",
    "test_probabilty = np.stack(test_prob)\n",
    "\n",
    "np.savez('KoBERT_'+str(dr_rate)[0]+str(dr_rate)[2]+'.npz',predict_prob=test_probabilty)\n",
    "\n",
    "# data = np.load('predict_prob.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCPagNP6qxgl"
   },
   "source": [
    "## Embadding Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBSgmxHGq1aV"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KrIrB59lq2na"
   },
   "outputs": [],
   "source": [
    "eatures = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        feature = model.extract_features(token_ids, valid_length, segment_ids)\n",
    "        features.append(feature.detach().cpu().numpy())\n",
    "\n",
    "features = np.concatenate(features, axis=0)\n",
    "\n",
    "# feature shape을 (6312, 64*768)으로 reshape\n",
    "features_2d = features.reshape((features.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "neyBdhJaq4q0"
   },
   "outputs": [],
   "source": [
    "# T-SNE 모델 생성 및 학습\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "features_tsne = tsne.fit_transform(features_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFNDWOawq8Fq"
   },
   "outputs": [],
   "source": [
    "#Embadding Visulaztion\n",
    "cmap = plt.cm.get_cmap('jet', 7)\n",
    "emotion_list = ['angry', 'disqust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "plt.scatter(features_tsne[:, 0], features_tsne[:, 1], c=test['Emotion'], cmap=cmap, s=5)\n",
    "\n",
    "# legend 달기\n",
    "for i, label in enumerate(np.unique(test['Emotion'])):\n",
    "    plt.scatter([], [], c=cmap(i), label=emotion_list[label], s=30)\n",
    "\n",
    "# legend 위치 조정\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n",
    "#plt.savefig('t-SNE_word_embadding visualization_test.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwLu1KI5q-_9"
   },
   "source": [
    "## Pretrained Model Embadding Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OL0D-3DrEU6"
   },
   "outputs": [],
   "source": [
    "no_train_bert,no_train_vocab = get_pytorch_kobert_model()\n",
    "\n",
    "no_train_model = BERTClassifier(no_train_bert,  dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oaRtW-pSrG9l"
   },
   "outputs": [],
   "source": [
    "eatures_1 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        feature = no_train_model.extract_features(token_ids, valid_length, segment_ids)\n",
    "        features_1.append(feature.detach().cpu().numpy())\n",
    "\n",
    "features_1 = np.concatenate(features_1, axis=0)\n",
    "\n",
    "# feature shape을 (6312, 64*768)으로 reshape\n",
    "features_2d_1 = features_1.reshape((features_1.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YYLQV1-ErHTm"
   },
   "outputs": [],
   "source": [
    "# T-SNE 모델 생성 및 학습\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "features_tsne_1 = tsne.fit_transform(features_2d_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-TzNIagrIeT"
   },
   "outputs": [],
   "source": [
    "#Embadding Visulaztion\n",
    "cmap = plt.cm.get_cmap('jet', 7)\n",
    "emotion_list = ['angry', 'disqust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "plt.scatter(features_tsne_1[:, 0], features_tsne_1[:, 1], c=test['Emotion'], cmap=cmap, s=5)\n",
    "\n",
    "# legend 달기\n",
    "for i, label in enumerate(np.unique(test['Emotion'])):\n",
    "    plt.scatter([], [], c=cmap(i), label=emotion_list[label], s=30)\n",
    "\n",
    "# legend 위치 조정\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n",
    "#plt.savefig('t-SNE_word_embadding visualization_pretrained_model_test.jpg')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
