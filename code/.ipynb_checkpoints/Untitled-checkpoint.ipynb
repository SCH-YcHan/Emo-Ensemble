{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1132507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e55e291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load(\"../../KEMDy20_v1_1/Extract/Dataset_MFCC.npz\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7d70376",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = dataset[\"train_x\"]\n",
    "train_y = dataset[\"train_y\"]\n",
    "test_x = dataset[\"test_x\"]\n",
    "test_y = dataset[\"test_y\"]\n",
    "train_aug_x = dataset[\"train_aug_x\"]\n",
    "train_aug_y = dataset[\"train_aug_y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "921b4076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Macro_f1(y_true, y_pred):\n",
    "    y_true = K.round(y_true)\n",
    "    y_pred = K.round(y_pred)\n",
    "    \n",
    "    tp = K.sum(y_true * y_pred, axis=0)\n",
    "    fp = K.sum((1-y_true) * y_pred, axis=0)\n",
    "    fn = K.sum(y_true * (1-y_pred), axis=0)\n",
    "    precision = tp / (tp + fp + K.epsilon())\n",
    "    recall = tp / (tp + fn + K.epsilon())\n",
    "    f1 = 2 * precision * recall / (precision + recall + K.epsilon())\n",
    "    \n",
    "    # Calculate macro F1 score\n",
    "    macro_f1 = K.mean(f1)\n",
    "    \n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "312eb411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# 레이블 인코딩\n",
    "le = LabelEncoder()\n",
    "le.fit(train_y)\n",
    "encoded_train_y = le.transform(train_y)\n",
    "\n",
    "# 원핫 인코딩\n",
    "onehot_train_y = to_categorical(encoded_train_y)\n",
    "\n",
    "# train_y에서 사용된 LabelEncoder 객체를 그대로 사용하여 test_y 인코딩\n",
    "encoded_test_y = le.transform(test_y)\n",
    "\n",
    "# test_y를 원핫 인코딩\n",
    "onehot_test_y = to_categorical(encoded_test_y)\n",
    "\n",
    "# Split the train and test data into four parts\n",
    "train_data_parts = np.split(train_x, 4, axis=1)\n",
    "test_data_parts = np.split(test_x, 4, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75fccea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, BatchNormalization, Flatten, Dense, Average, Dropout\n",
    "\n",
    "# Define submodel architecture\n",
    "def create_submodel(input_shape):\n",
    "    submodel_input = Input(shape=input_shape)\n",
    "    x = Conv1D(filters=16, kernel_size=3, activation='relu')(submodel_input)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(filters=32, kernel_size=3, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(units=256, activation='relu')(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(units=7, activation='softmax')(x)\n",
    "    return submodel_input, x\n",
    "\n",
    "# Define the list to store submodels and input shapes\n",
    "submodels = []\n",
    "input_shapes = []\n",
    "\n",
    "# Create submodels\n",
    "for i in range(4):\n",
    "    input_shape = train_data_parts[i].shape[1:]\n",
    "    submodel_input, submodel_output = create_submodel(input_shape)\n",
    "    submodels.append(Model(inputs=submodel_input, outputs=submodel_output))\n",
    "    input_shapes.append(input_shape)\n",
    "\n",
    "# Define final model architecture\n",
    "inputs = [Input(shape=input_shape) for input_shape in input_shapes]\n",
    "outputs = [[submodel(inputs[i])] for i, submodel in enumerate(submodels)]\n",
    "merged = Average()([output[0] for output in outputs])\n",
    "#merged = Dense(units=7, activation='softmax')(Flatten()(merged))\n",
    "final_model = Model(inputs=inputs, outputs=merged)\n",
    "\n",
    "# Compile the final model\n",
    "final_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[Macro_f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7fd0b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "# Visualize final model architecture\n",
    "plot_model(final_model, to_file='../final_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bb7bc2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "931/931 [==============================] - 33s 26ms/step - loss: 1.5148 - Macro_f1: 0.1136 - val_loss: 0.6081 - val_Macro_f1: 0.1237\n",
      "Epoch 2/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 1.4017 - Macro_f1: 0.1140 - val_loss: 0.6114 - val_Macro_f1: 0.1254\n",
      "Epoch 3/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 1.3689 - Macro_f1: 0.1206 - val_loss: 0.5850 - val_Macro_f1: 0.1265\n",
      "Epoch 4/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 1.3425 - Macro_f1: 0.1275 - val_loss: 0.5735 - val_Macro_f1: 0.1296\n",
      "Epoch 5/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 1.3204 - Macro_f1: 0.1368 - val_loss: 0.5992 - val_Macro_f1: 0.1300\n",
      "Epoch 6/100\n",
      "931/931 [==============================] - 24s 25ms/step - loss: 1.2920 - Macro_f1: 0.1472 - val_loss: 0.5801 - val_Macro_f1: 0.1512\n",
      "Epoch 7/100\n",
      "931/931 [==============================] - 24s 25ms/step - loss: 1.2685 - Macro_f1: 0.1554 - val_loss: 0.5839 - val_Macro_f1: 0.1299\n",
      "Epoch 8/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 1.2394 - Macro_f1: 0.1717 - val_loss: 0.5774 - val_Macro_f1: 0.1369\n",
      "Epoch 9/100\n",
      "931/931 [==============================] - 24s 25ms/step - loss: 1.2064 - Macro_f1: 0.1848 - val_loss: 0.5885 - val_Macro_f1: 0.1409\n",
      "Epoch 10/100\n",
      "931/931 [==============================] - 24s 25ms/step - loss: 1.1747 - Macro_f1: 0.1990 - val_loss: 0.5916 - val_Macro_f1: 0.1441\n",
      "Epoch 11/100\n",
      "931/931 [==============================] - 24s 25ms/step - loss: 1.1398 - Macro_f1: 0.2113 - val_loss: 0.5855 - val_Macro_f1: 0.1474\n",
      "Epoch 12/100\n",
      "931/931 [==============================] - 24s 25ms/step - loss: 1.1044 - Macro_f1: 0.2256 - val_loss: 0.6331 - val_Macro_f1: 0.1494\n",
      "Epoch 13/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 1.0792 - Macro_f1: 0.2389 - val_loss: 0.5982 - val_Macro_f1: 0.1509\n",
      "Epoch 14/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 1.0434 - Macro_f1: 0.2519 - val_loss: 0.6322 - val_Macro_f1: 0.1405\n",
      "Epoch 15/100\n",
      "931/931 [==============================] - 24s 25ms/step - loss: 1.0208 - Macro_f1: 0.2619 - val_loss: 0.6665 - val_Macro_f1: 0.1490\n",
      "Epoch 16/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.9936 - Macro_f1: 0.2733 - val_loss: 0.6341 - val_Macro_f1: 0.1422\n",
      "Epoch 17/100\n",
      "931/931 [==============================] - 24s 25ms/step - loss: 0.9721 - Macro_f1: 0.2913 - val_loss: 0.6513 - val_Macro_f1: 0.1560\n",
      "Epoch 18/100\n",
      "931/931 [==============================] - 24s 25ms/step - loss: 0.9452 - Macro_f1: 0.2996 - val_loss: 0.6839 - val_Macro_f1: 0.1503\n",
      "Epoch 19/100\n",
      "931/931 [==============================] - 24s 25ms/step - loss: 0.9148 - Macro_f1: 0.3169 - val_loss: 0.7197 - val_Macro_f1: 0.1472\n",
      "Epoch 20/100\n",
      "931/931 [==============================] - 24s 25ms/step - loss: 0.8824 - Macro_f1: 0.3336 - val_loss: 0.6957 - val_Macro_f1: 0.1516\n",
      "Epoch 21/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.8618 - Macro_f1: 0.3473 - val_loss: 0.6770 - val_Macro_f1: 0.1544\n",
      "Epoch 22/100\n",
      "931/931 [==============================] - 24s 25ms/step - loss: 0.8426 - Macro_f1: 0.3562 - val_loss: 0.7637 - val_Macro_f1: 0.1527\n",
      "Epoch 23/100\n",
      "931/931 [==============================] - 23s 24ms/step - loss: 0.8288 - Macro_f1: 0.3620 - val_loss: 0.7054 - val_Macro_f1: 0.1492\n",
      "Epoch 24/100\n",
      "931/931 [==============================] - 23s 24ms/step - loss: 0.8110 - Macro_f1: 0.3748 - val_loss: 0.7417 - val_Macro_f1: 0.1537\n",
      "Epoch 25/100\n",
      "931/931 [==============================] - 23s 25ms/step - loss: 0.7916 - Macro_f1: 0.3891 - val_loss: 0.7599 - val_Macro_f1: 0.1550\n",
      "Epoch 26/100\n",
      "931/931 [==============================] - 23s 25ms/step - loss: 0.7775 - Macro_f1: 0.3935 - val_loss: 0.7468 - val_Macro_f1: 0.1548\n",
      "Epoch 27/100\n",
      "931/931 [==============================] - 24s 25ms/step - loss: 0.7651 - Macro_f1: 0.4032 - val_loss: 0.7718 - val_Macro_f1: 0.1557\n",
      "Epoch 28/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.7578 - Macro_f1: 0.4119 - val_loss: 0.7707 - val_Macro_f1: 0.1528\n",
      "Epoch 29/100\n",
      "931/931 [==============================] - 24s 25ms/step - loss: 0.7437 - Macro_f1: 0.4217 - val_loss: 0.8455 - val_Macro_f1: 0.1528\n",
      "Epoch 30/100\n",
      "931/931 [==============================] - 22s 24ms/step - loss: 0.7312 - Macro_f1: 0.4213 - val_loss: 0.8671 - val_Macro_f1: 0.1549\n",
      "Epoch 31/100\n",
      "931/931 [==============================] - 23s 24ms/step - loss: 0.7149 - Macro_f1: 0.4393 - val_loss: 0.8051 - val_Macro_f1: 0.1553\n",
      "Epoch 32/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.7063 - Macro_f1: 0.4483 - val_loss: 0.7977 - val_Macro_f1: 0.1519\n",
      "Epoch 33/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.6970 - Macro_f1: 0.4539 - val_loss: 0.8362 - val_Macro_f1: 0.1527\n",
      "Epoch 34/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.6957 - Macro_f1: 0.4554 - val_loss: 0.8557 - val_Macro_f1: 0.1509\n",
      "Epoch 35/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.6780 - Macro_f1: 0.4674 - val_loss: 0.9154 - val_Macro_f1: 0.1472\n",
      "Epoch 36/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.6755 - Macro_f1: 0.4733 - val_loss: 0.8301 - val_Macro_f1: 0.1554\n",
      "Epoch 37/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.6578 - Macro_f1: 0.4804 - val_loss: 0.9646 - val_Macro_f1: 0.1517\n",
      "Epoch 38/100\n",
      "931/931 [==============================] - 24s 25ms/step - loss: 0.6662 - Macro_f1: 0.4842 - val_loss: 0.8752 - val_Macro_f1: 0.1528\n",
      "Epoch 39/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.6623 - Macro_f1: 0.4810 - val_loss: 0.8047 - val_Macro_f1: 0.1524\n",
      "Epoch 40/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.6472 - Macro_f1: 0.4929 - val_loss: 0.9213 - val_Macro_f1: 0.1461\n",
      "Epoch 41/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.6459 - Macro_f1: 0.4940 - val_loss: 0.8744 - val_Macro_f1: 0.1476\n",
      "Epoch 42/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.6418 - Macro_f1: 0.4998 - val_loss: 0.9741 - val_Macro_f1: 0.1498\n",
      "Epoch 43/100\n",
      "931/931 [==============================] - 25s 26ms/step - loss: 0.6333 - Macro_f1: 0.4989 - val_loss: 0.9658 - val_Macro_f1: 0.1526\n",
      "Epoch 44/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.6275 - Macro_f1: 0.5115 - val_loss: 0.9072 - val_Macro_f1: 0.1519\n",
      "Epoch 45/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.6238 - Macro_f1: 0.5161 - val_loss: 0.9127 - val_Macro_f1: 0.1529\n",
      "Epoch 46/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.6120 - Macro_f1: 0.5197 - val_loss: 0.9218 - val_Macro_f1: 0.1554\n",
      "Epoch 47/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.6102 - Macro_f1: 0.5199 - val_loss: 1.0045 - val_Macro_f1: 0.1538\n",
      "Epoch 48/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.6073 - Macro_f1: 0.5259 - val_loss: 0.9526 - val_Macro_f1: 0.1500\n",
      "Epoch 49/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.6120 - Macro_f1: 0.5217 - val_loss: 1.0341 - val_Macro_f1: 0.1511\n",
      "Epoch 50/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.5969 - Macro_f1: 0.5340 - val_loss: 0.9246 - val_Macro_f1: 0.1535\n",
      "Epoch 51/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.5943 - Macro_f1: 0.5372 - val_loss: 0.9459 - val_Macro_f1: 0.1549\n",
      "Epoch 52/100\n",
      "931/931 [==============================] - 24s 25ms/step - loss: 0.5861 - Macro_f1: 0.5535 - val_loss: 0.9573 - val_Macro_f1: 0.1489\n",
      "Epoch 53/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.5861 - Macro_f1: 0.5475 - val_loss: 0.8902 - val_Macro_f1: 0.1566\n",
      "Epoch 54/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.5733 - Macro_f1: 0.5575 - val_loss: 0.9427 - val_Macro_f1: 0.1577\n",
      "Epoch 55/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.5908 - Macro_f1: 0.5485 - val_loss: 0.9654 - val_Macro_f1: 0.1500\n",
      "Epoch 56/100\n",
      "931/931 [==============================] - 24s 26ms/step - loss: 0.5790 - Macro_f1: 0.5584 - val_loss: 0.9314 - val_Macro_f1: 0.1544\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/931 [=================>............] - ETA: 9s - loss: 0.5826 - Macro_f1: 0.5617"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "\n",
    "# Prepare the data for training\n",
    "final_train_data = [train_data_parts[i].reshape(train_x.shape[0], -1, train_aug_x.shape[2]) for i in range(4)]\n",
    "final_test_data = [test_data_parts[i].reshape(test_x.shape[0], -1, test_x.shape[2]) for i in range(4)]\n",
    "\n",
    "weight_path=\"../Model/Model_weights_best.hdf5\"\n",
    "checkpoint = callbacks.ModelCheckpoint(\n",
    "    weight_path,\n",
    "    monitor='val_Macro_f1',\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# Train the final model\n",
    "final_model.fit(\n",
    "    final_train_data,\n",
    "    onehot_train_y,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(final_test_data, onehot_test_y)\n",
    ")\n",
    "\n",
    "# Evaluate the final model on the test data\n",
    "test_loss, test_macro_f1 = final_model.evaluate(final_test_data, onehot_test_y)\n",
    "print(f'Test loss: {test_loss}, test macro_f1: {test_macro_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80e839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_weights(weight_path)\n",
    "best_eva_list = model.evaluate(final_test_data, onehot_test_y)\n",
    "loss = best_eva_list[0]\n",
    "macro_f1 = best_eva_list[1]\n",
    "print('Model evaluation: ', best_eva_list)\n",
    "y_pred_best = self.model.predict(tx)\n",
    "self.matrix.append(confusion_matrix(np.argmax(ty,axis=1),np.argmax(y_pred_best,axis=1)))\n",
    "em = classification_report(np.argmax(ty,axis=1), np.argmax(y_pred_best,axis=1), target_names=self.class_label, output_dict=True)\n",
    "self.eva_matrix.append(em)\n",
    "print(classification_report(np.argmax(ty,axis=1), np.argmax(y_pred_best,axis=1), target_names=self.class_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbc5445c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.manifold import TSNE\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#tsne = TSNE(n_components=2, random_state=0)\n",
    "#digits_tsne = tsne.fit_transform(train_aug_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5ecb259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10, 8))\n",
    "#plt.scatter(digits_tsne[:, 0], digits_tsne[:, 1])\n",
    "#plt.colorbar()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d021f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91efdc06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
